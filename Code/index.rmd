---
title: "Gaussian Mixture Models (GMM)"
subtitle: "Fitting GMMs using the Expectation-Maximization (E-M) Algorithm in R"  
author: 
  - "Pavitra Chakravarty"
date: '2020-09-13'
output:
  xaringan::moon_reader:
    lib_dir: assets
    chakra: assets/remark-0.14.0.min.js
    css: [xaringan-themer.css, xaringan-extra.css]
    seal: false
    nature:
      highlightStyle: atom-one-light
      highlightLines: true
      ratio: "16:9"
      countIncrementalSlides: true
      slideNumberFormat: "%current%"

bibliography: refs.bib
biblio-style: apalike
link-citations: yes
---

class: title-slide middle

# `r rmarkdown::metadata$title`

## `r rmarkdown::metadata$subtitle`

.author[
`r rmarkdown::metadata$author`
]

.twitter[
@genomixgmailcom
]

<img src="images/MLT Logo black background.png" class="user-logo" />

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r global.options, include = FALSE}
knitr::opts_chunk$set(
    fig.width   = 15,       # the width for plots created by code chunk
    fig.height  = 8,        # the height for plots created by code chunk
    fig.align   = 'center', # how to align graphics in the final doc. 'left', 'right', 'center'
    fig.path    = 'figs/',  # file path to the directory where knitr shall store the graphics files
    results     = 'asis',   # knitr will pass through results without reformatting them
    echo        = TRUE,     # in FALSE knitr will not display code in the code chunk above it's results
    message     = TRUE,     # if FALSE knitr will not display any messages generated by code
    strip.white = TRUE,     # if FALSE knitr will not remove white spaces at the beg or end of code chunk
    warning     = FALSE)    # if FALSE knitr will not display any warning messages in the final document
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
library(tidyverse)
library(ggplot2)
library(palmerpenguins)
library(mixtools)

style_duo_accent(
  base_font_size = "22px",
  primary_color = "#1381B0",
  secondary_color = "#FF961C",
  inverse_header_color = "#FFFFFF",
  title_slide_text_color = "#FFFFFF",
  code_inline_font_size = "1em",
  extra_fonts = list(
    google_font("Staatliches"),
    google_font("Megrim"),
    google_font("Pompiere")
  )
)

```

```{r setup extra, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE,
                      comment = "")
xaringanExtra::use_tile_view()
xaringanExtra::use_panelset()
xaringanExtra::use_webcam()
xaringanExtra::use_editable()
xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         
  mute_unhighlighted_code = TRUE  
)
```
---


class: middle

# What are Gaussian Mixture Models (GMM)? 


* A mixture model is a mixture of k component distributions that collectively make a mixture distribution $f(x)$:

$$f(x) = \sum_{k=1}^{K}\alpha_{k}f_{k}(x)$$
* $\alpha_{k}$: mixing weight for the $k^{th}$ component where $\sum_{k=1}^{K}\alpha_{k} = 1$

* $f_k(x)$:     any sort of distribution (preferably, parametric). If you choose gaussian, you get GMM

* $\theta_{k}$: $$f(x) = \sum_{k=1}^{K}\alpha_{k}f_{k}(x;\theta_{k})$$

---

class: middle

# Features of GMMs

$$ \mathcal{N}(\mu, \sigma^2) $$

The normal distribution is parameterized by two variables:

* $\mu$: Mean; Center of the mass

* $\sigma^2$: Variance; Spread of the mass

* Entire data can be represented as a k-mixture GMM


---

# Meet the Palmer penguins

```{r, echo = FALSE, fig.align='center'}
knitr::include_graphics("figs/lter_penguins.png", dpi = 300)
```

# <a href='https://allisonhorst.github.io/palmerpenguins'><img src='figs/palmerpenguins.png' align="right" height="138.5" /></a>

---

```{r, eval = TRUE, echo=FALSE, fig.align='center'}
knitr::include_graphics("figs/all_peng-1.png", dpi = 100)
```

# <a href='https://cran.r-project.org/web/packages/explore/vignettes/explore_penguins.html'><img src='figs/culmen_depth.png' align="right" height="60.5" /></a>


---

class: middle

# "Complete Data" Scenario

---

class: bigger

.panelset[
.panel[.panel-name[Penguins]

```{r all_peng, eval=TRUE , fig.show='hide'}
flipper_bill <- ggplot(data = penguins,
                         aes(x = flipper_length_mm,
                             y = bill_length_mm)) +
  geom_point(aes(color = species, 
                 shape = species),
             size = 3,
             alpha = 0.8) +
  theme_minimal() +
  scale_color_manual(values = c("darkorange","purple","cyan4")) +
  labs(title = "Flipper and bill length",
       subtitle = "Dimensions for Adelie, Chinstrap and Gentoo Penguins at Palmer Station LTER",
       x = "Flipper length (mm)",
       y = "Bill length (mm)",
       color = "Penguin species",
       shape = "Penguin species") +
  theme(legend.position = c(0.85, 0.15),
        legend.background = element_rect(fill = "white", color = NA),
        plot.title.position = "plot",
        plot.caption = element_text(hjust = 0, face= "italic"),
        plot.caption.position = "plot")

flipper_bill

```
]


.panel[.panel-name[Plot]
```{r, eval = TRUE, echo=FALSE}
knitr::include_graphics("figs/README-flipper-hist-1.png", dpi = 225)
```
]
]
---

class: middle

# "Incomplete Data" Scenario

---

class: bigger

.panelset[
.panel[.panel-name[Density of Flipper Length]

```{r peng_flipper_len_density, eval=TRUE , fig.show='hide'}
flipper_len_density <- ggplot(data = penguins,
                         aes(x = flipper_length_mm)) +
  geom_density() +
  theme_minimal() +
  
  labs(title = "Flipper length density",
       subtitle = "Dimensions for Adelie, Chinstrap and Gentoo Penguins at Palmer Station LTER",
       x = "Flipper length (mm)",
       y = "Density",
       color = "Penguin species",
       shape = "Penguin species") +
  theme(legend.position = c(0.85, 0.15),
        legend.background = element_rect(fill = "white", color = NA),
        plot.title.position = "plot",
        plot.caption = element_text(hjust = 0, face= "italic"),
        plot.caption.position = "plot") 

flipper_len_density <- flipper_len_density +
  geom_vline(xintercept = 192, col = "red", size = 2) + 
  geom_vline(xintercept = 215, col = "blue", size = 2) 

flipper_len_density
```
]


.panel[.panel-name[Plot]
![](`r knitr::fig_chunk("peng_flipper_len_density", "png") `)
]
]

---

class: middle

# Black-box approach to get GMM

---

class: bigger

.panelset[
.panel[.panel-name[MixEM_Blackbox_code]

```{r code, eval=TRUE , fig.show='hide'}
plot_mix_comps <- function(x, mu, sigma, lam) {
  lam * dnorm(x, mu, sigma)
}

set.seed(1)
wait <- penguins$flipper_length_mm
wait <- na.omit(penguins$flipper_length_mm)
mixmdl <- normalmixEM(wait, k = 2)

data.frame(x = mixmdl$x) %>%
  ggplot() +
  geom_histogram(aes(x, ..density..), binwidth = 1, colour = "black", 
                 fill = "white") +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(mixmdl$mu[1], mixmdl$sigma[1], lam = mixmdl$lambda[1]),
                colour = "red", lwd = 1.5) +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(mixmdl$mu[2], mixmdl$sigma[2], lam = mixmdl$lambda[2]),
                colour = "blue", lwd = 1.5) +
  ylab("Density")

```
]

.panel[.panel-name[Output 1]

```{r MixEM_Blackbox_1, eval=TRUE , fig.show='hide'}

mixmdl$lambda

mixmdl$mu

mixmdl$sigma

```
]

.panel[.panel-name[Output 2]

```{r MixEM_Blackbox_2, eval=TRUE , fig.show='hide'}

mixmdl$loglik

head(mixmdl$posterior, n = 25L)
```
]

.panel[.panel-name[Plot]
![](`r knitr::fig_chunk("code", "png") `)
]
]

---

class: middle

# Refresher

---


# K-means algorithm

Goal: represent dataset as K clusters, each summarized by point estimate:$$\mu$$

Initialize prototypes and iterate between:

* E-step: assign data point to nearest prototype

* M-step: update prototypes to be cluster means

* Minimize Cost function

```{r, eval = TRUE, echo=FALSE}
knitr::include_graphics("figs/Kmeans_loss.png", dpi = 150)
```

---

# Gaussian Probabilistic Clustering

Goal: represent probability distribution of data as ***Mixture Model***

* Mixtures of Gaussians

* Capture uncertainty in cluster assignments

* Maximize Likelihood

$$P(X|\mu,\sigma,\alpha) = \sum_{k=1}^{K}\alpha_k\mathcal{N}(X|\mu_{k},\sigma_{k}^{2})$$
$$P(X|\mu,\sigma,\alpha) = \alpha_1\mathcal{N}(X|\mu_{1},\sigma_{1}^{2}) +  \alpha_2\mathcal{N}(X|\mu_{2},\sigma_{2}^{2})$$

---

class: middle


The expectation maximization algorithm is a natural generalization of maximum likelihood estimation to the incomplete data case

Chuong B Do & Serafim Batzoglou. 

*What is the expectation maximization algorithm? Nature Biotechnology. 2008*

---

.pull-left[

### Maximum Likelihood Estimation

$\mu_{k} = \frac{\sum_{i}^{N_{k}}x_{i,k}}{N_{k}}$

$\sigma_{k} = \frac{\sum_{i}^{N_{k}}(x_{i,k} - \mu_{k})^2}{N_{k}}$
  
$\alpha_{k} = \frac{N_{k}}{N}$
  
]

.pull-right[

### Expectation Maximization

GMM equation:

$$P(X|\mu,\sigma,\alpha) = \sum_{k=1}^{K}\alpha_k\mathcal{N}(X|\mu_{k},\sigma_{k}^{2})$$

Bayes' Rule:

$$P(x_{i} \in k_{j} | x_{i}) = \frac{P(x_{i} | x_{i} \in k_{j})P(k_{j})}{P(x_{i})}$$
What we are interested in is $P(x_{i} \in k_{j} | x_{i})$ "posterior probability" 

***Expectation***: calculate "posterior probability"

***Maximization***: re-estimate parameters using variant of MLE

]

---

```{r, eval = TRUE, echo=FALSE}
knitr::include_graphics("figs/NBT_EM.png", dpi = 115)
```

---

class: middle

# How do you test if MLE/EM performed well?

***USING LOSS FUNCTION***

---

.pull-left[

### MLE

Minimize Loss

```{r, eval = TRUE, echo=FALSE}
knitr::include_graphics("figs/Kmeans_loss.png", dpi = 150)
```
  
]

.pull-right[

### E-M

Maximize Log Likelihood


```{r, eval = TRUE, echo=FALSE}
knitr::include_graphics("figs/EM_loss.png", dpi = 50)
```

]

---

class: middle

# What is going on inside the E-M?

## Imagine when our data is 'unlabeled' or 'incomplete'

---

class: bigger

.panelset[
.panel[.panel-name[EM_Initial]

```{r EM_init, eval=TRUE , fig.show='hide'}
wait <- na.omit(penguins$flipper_length_mm)

wait.kmeans <- kmeans(wait, 2)
wait.kmeans.cluster <- wait.kmeans$cluster

wait.df <- data_frame(x = wait, cluster = wait.kmeans.cluster)

wait1.df <- wait.df %>%
  mutate(num = row_number()) %>%
  ggplot(aes(y = num, x = x, color = factor(cluster))) +
  geom_point() +
  ylab("Values") +
  ylab("Data Point Number") +
  scale_color_discrete(name = "Cluster") +
  ggtitle("K-means Clustering")

wait1.df
```
]

.panel[.panel-name[Plot]
![](`r knitr::fig_chunk("EM_init", "png") `)
]
]

---

class: middle

# E-M: "Initialization" step

---

class: bigger

.panelset[
.panel[.panel-name[code]

```{r code_EM_init, eval=TRUE , fig.show='hide'}
 wait.summary.df <-  wait.df %>%
  group_by(cluster) %>%
  summarize(mu = mean(x), variance = var(x), std = sd(x), size = n(), .groups = 'drop') 
wait.summary.df <- wait.summary.df %>%
  mutate(alpha = size / sum(size))

```
]

.panel[.panel-name[Output]

```{r code_EM_init_output, eval=TRUE , fig.show='hide'}

wait.summary.df$cluster

wait.summary.df$size

wait.summary.df$alpha
```
]
]

---

class: middle

# E-M: "Expectation" step

---

class: bigger

.panelset[
.panel[.panel-name[code]

```{r code_EM_Expect, eval=TRUE , fig.show='hide'}
e_step <- function(x, mu.vector, sd.vector, alpha.vector) {
  comp1.prod <- dnorm(x, mu.vector[1], sd.vector[1]) * alpha.vector[1]
  comp2.prod <- dnorm(x, mu.vector[2], sd.vector[2]) * alpha.vector[2]
  sum.of.comps <- comp1.prod + comp2.prod
  comp1.post <- comp1.prod / sum.of.comps
  comp2.post <- comp2.prod / sum.of.comps

  sum.of.comps.ln <- log(sum.of.comps, base = exp(1))
  sum.of.comps.ln.sum <- sum(sum.of.comps.ln)

  list("loglik" = sum.of.comps.ln.sum,
       "posterior.df" = cbind(comp1.post, comp2.post))
}
```
]

.panel[.panel-name[Output]

```{r code_EM_Expect_output, eval=TRUE, echo = FALSE}
knitr::include_graphics("figs/posterior_df.png", dpi =  75)
```
]
]

---

class: middle

# E-M: "Maximization" step

---

class: bigger

.panelset[
.panel[.panel-name[code]

```{r code_EM_Maximization, eval=TRUE , fig.show='hide'}
m_step <- function(x, posterior.df) {
  comp1.n <- sum(posterior.df[, 1])
  comp2.n <- sum(posterior.df[, 2])

  comp1.mu <- 1/comp1.n * sum(posterior.df[, 1] * x)
  comp2.mu <- 1/comp2.n * sum(posterior.df[, 2] * x)

  comp1.var <- sum(posterior.df[, 1] * (x - comp1.mu)^2) * 1/comp1.n
  comp2.var <- sum(posterior.df[, 2] * (x - comp2.mu)^2) * 1/comp2.n

  comp1.alpha <- comp1.n / length(x)
  comp2.alpha <- comp2.n / length(x)

  list("mu" = c(comp1.mu, comp2.mu),
       "var" = c(comp1.var, comp2.var),
       "alpha" = c(comp1.alpha, comp2.alpha))
}

```

]

.panel[.panel-name[EM-Loop]

```{r code_EM_Loop, eval=TRUE , fig.show='hide'}

for (i in 1:50) {
  if (i == 1) {
    # Initialization
    e.step <- e_step(wait, wait.summary.df[["mu"]], wait.summary.df[["std"]],
                     wait.summary.df[["alpha"]])
    m.step <- m_step(wait, e.step[["posterior.df"]])
    cur.loglik <- e.step[["loglik"]]
    loglik.vector <- e.step[["loglik"]]
  } else {
    # Repeat E and M steps till convergence
    e.step <- e_step(wait, m.step[["mu"]], sqrt(m.step[["var"]]), 
                     m.step[["alpha"]])
    m.step <- m_step(wait, e.step[["posterior.df"]])
    loglik.vector <- c(loglik.vector, e.step[["loglik"]])

    loglik.diff <- abs((cur.loglik - e.step[["loglik"]]))
    if(loglik.diff < 1e-6) {
      break
    } else {
      cur.loglik <- e.step[["loglik"]]
    }
  }
}

loglik.vector


```
]

.panel[.panel-name[Output]

```{r code_EM_Maximization_output, eval=TRUE, echo = FALSE}
knitr::include_graphics("figs/loglik.png", dpi =  75)
```
]
]

---

class: middle

# Final GMM from running E-M

---

class: bigger

.panelset[
.panel[.panel-name[code]

```{r code_EM_GMM, eval=TRUE , fig.show='hide'}
plot_mix_comps <- function(x, mu, sigma, lam) {
  lam * dnorm(x, mu, sigma)
}

final_gmm_plot <- data.frame(x = wait) %>%
  ggplot() +
  geom_histogram(aes(x, ..density..), binwidth = 1, colour = "black", 
                 fill = "white") +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(m.step$mu[1], sqrt(m.step$var[1]), 
                           lam = m.step$alpha[1]),
                colour = "red", lwd = 1.5) +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(m.step$mu[2], sqrt(m.step$var[2]), 
                           lam = m.step$alpha[2]),
                colour = "blue", lwd = 1.5) +
  ylab("Density") +
  xlab("Values") +
  ggtitle("Final GMM Fit")

final_gmm_plot
```

]

.panel[.panel-name[Plot]
![](`r knitr::fig_chunk("code_EM_GMM", "png") `)
]
]

---

class: middle

# Acknowledgments

### I would like to specially acknowledge the work of [***Dr. Fong Chun Chan***](https://tinyheero.github.io/about/). He had already developed all the code that was used in these slides. 
### I only adapted his code to work with the [***penguins***](https://education.rstudio.com/blog/2020/07/palmerpenguins-cran/) dataset

---

# References

```{r, load_refs, echo=FALSE}
library(RefManageR)
bib <- ReadBib("refs.bib", check = FALSE)
ui <- "- "
```

```{r, print_refs, results='asis', echo=FALSE, warning=FALSE, message=FALSE}
writeLines(ui)
print(bib[key = "FongEMAlgo"], 
  .opts = list(check.entries = FALSE, 
               style = "html", 
               bib.style = "authoryear"))
writeLines(ui)
print(bib[key = "FongKMeans"], 
  .opts = list(check.entries = FALSE, 
               style = "html", 
               bib.style = "authoryear"))
writeLines(ui)
print(bib[key = "GarrickSlidesExtra"], 
  .opts = list(check.entries = FALSE, 
               style = "html", 
               bib.style = "authoryear"))
writeLines(ui)
print(bib[key = "AllisonPP"], 
  .opts = list(check.entries = FALSE, 
               style = "html", 
               bib.style = "authoryear"))
writeLines(ui)
print(bib[key = "RolandKrasser"], 
  .opts = list(check.entries = FALSE, 
               style = "html", 
               bib.style = "authoryear"))
writeLines(ui)
print(bib[key = "ChuongSerfim"], 
  .opts = list(check.entries = FALSE, 
               style = "html", 
               bib.style = "authoryear"))

```

---

layout: false
class: middle left

<img src="images/userlogo.png" class="user-logo top-1" />

<p class="pompiere f1">&#x1F31F; <a href="https://github.com/genomix"><span class="gray">github.com</span>/genomix</a></p>

--

<p class="pompiere f1">&#x1F5E3; <a href="https://twitter.com/genomixgmailcom">&commat;genomix</a></p>

--

<p class="pompiere f1">&#x1F468;&#x1F3FC;&#x200D;&#x1F4BB; <a href="https://genomix.netlify.app">genomix.netlify.app</a></p>
